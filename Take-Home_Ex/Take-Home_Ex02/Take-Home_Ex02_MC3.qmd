---
title: "Take-Home Assignment 2"
description: ""
author: "David Chiam"
date: "22 May 2025"
date-modified: "31 May 2025"
format: html
editor: visual
execute: 
  eval: true
  echo: true
  warning: false
  freeze: true
---

# **Take Home Assignment 2 on Mini-Challenge 3**

## **1 - Getting Started**

### 1.1 - Load the R packages for this assignment

For the purpose of this assignment, five R packages will be used. They are tidyverse, jsonlite, tidygraph, ggraph and SmartEDA.

In the code chunk below, p_load() of pacman package is used to load the R packages into R environment.

```{r}

pacman::p_load(tidyverse, jsonlite, 
               tidygraph, ggraph, SmartEDA, 
               ggrepel, scales, lubridate, dplyr, viridis)
```

### 1.2 - Importing Knowledge Graph Data

For the purpose of this exercise, *mc3.json* file will be used. Before getting started, you should have the data set in the **data** sub-folder.

In the code chunk below, `fromJSON()` of **jsonlite** package is used to import *mc3.json* file into R and save the output object

```{r}

MC3 <- fromJSON("data/MC3_graph.json")
MC3_schema <- fromJSON("data/MC3_schema.json")
```

### 1.3 - **Data Overview**

The dataset was provided by VAST Challenge for MC3. This report utilizes two core datasets: MC3_graph.json, which encodes the knowledge graph of communications, events, and relationships; and MC3_schema.json, which defines the structure, subtypes, and attributes of each node and edge type within the graph. There ngraph contains a total of 1159 nodes and 3226 edges. Full description of node attributes and edge attributes is shown below.

**Nodes Attributes** are as such:

![Node Subtypes](Images/Node%20Attributes.png){fig-align="center" width="600"}

**Edge Attributes** are as such:

![Node-Edge-Node Matrix](Images/Edge%20Attributes.png){fig-align="center" width="594"}

### 1.4 - **Inspecting knowledge graph structure**

Before preparing the data, it is always a good practice to examine the structure of *mc3* knowledge graph.

In the code chunk below `glimpse()` is used to reveal the structure of *mc3* knowledge graph.

```{r}

glimpse(MC3)
```

::: callout-warning
Notice that¬†*Industry*¬†field is in list data type. In general, this data type is not acceptable by¬†`tbl_graph()`¬†of¬†**tidygraph**. In order to avoid error arise when building tidygraph object, it is wiser to exclude this field from the edges data table. However, it might be still useful in subsequent analysis.
:::

### 1.5 - Extract the edges and nodes tables

Next,¬†`as_tibble()`¬†of¬†**tibble**¬†package package is used to extract the nodes and links tibble data frames from¬†*mc3*¬†tibble dataframe into two separate tibble dataframes called¬†*mc3_nodes*¬†and¬†*mc3_edges*¬†respectively.

```{r}

mc3_nodes <- as_tibble(MC3$nodes)
mc3_edges <- as_tibble(MC3$edges)
```

### 1.6 - Brief Analyis of the extracted edges and nodes data (visuals)

It is time for us to apply appropriate EDA methods to examine the data.

**Nodes:**

:::: panel-tabset
In the code chunk below, `ExpCatViz()` of SmartEDA package is used to reveal the frequency distribution of all categorical fields in *mc3_nodes* tibble dataframe.

### The Code Chunk

```{r, echo=TRUE, eval=FALSE}

ExpCatViz(data=mc3_nodes,
          col="lightblue")
```

### The Plots

```{r}

ExpCatViz(data=mc3_nodes,
          col="lightblue")
```

::: callout-note
What useful discovery can you obtained from the visualisation above?
:::
::::

**Edges:**

:::: panel-tabset
On the other hands, code chunk below uses¬†`ExpCATViz()`¬†of SmartEDA package to reveal the frequency distribution of all categorical fields in¬†*mc3_edges*¬†tibble dataframe.

### The Code Chunk

```{r, echo=TRUE, eval=FALSE}

ExpCatViz(data=mc3_edges,
          col="lightblue")
```

### The Plots

```{r, echo=FALSE}

ExpCatViz(data=mc3_edges,
          col="lightblue")
```

::: callout-note
What useful discovery can you obtained from the visualisation above?
:::
::::

## 2 - **Data Cleaning and Wrangling**

### **2.1 - Cleaning and wrangling nodes**

Code chunk below performs the following data cleaning tasks:

-   convert values in id field into character data type,
-   exclude records with `id` value are na,
-   exclude records with similar id values,
-   exclude `thing_collected` field, and
-   save the cleaned tibble dataframe into a new tibble datatable called `mc3_nodes_cleaned`.

```{r}
#| code-fold: true

mc3_nodes_cleaned <- mc3_nodes %>%
  mutate(id = as.character(id)) %>%
  filter(!is.na(id)) %>%
  distinct(id, .keep_all = TRUE) %>%
  select(-thing_collected)
```

### **2.2 - Cleaning and wrangling edges**

Next, the code chunk below will be used to:

-   rename source and target fields to from_id and to_id respectively,
-   convert values in from_id and to_id fields to character data type,
-   exclude values in from_id and to_id which not found in the id field of mc3_nodes_cleaned,
-   exclude records whereby from_id and/or to_id values are missing, and
-   save the cleaned tibble dataframe and called it mc3_edges_cleaned.

```{r}
#| code-fold: true

mc3_edges_cleaned <- mc3_edges %>%
  rename(from_id = source, 
         to_id = target) %>%
  mutate(across(c(from_id, to_id), 
                as.character)) %>%
  filter(from_id %in% mc3_nodes_cleaned$id, 
         to_id %in% mc3_nodes_cleaned$id) %>%
  filter(!is.na(from_id), !is.na(to_id))
```

Next, code chunk below will be used to create mapping of character id in¬†`mc3_nodes_cleaned`¬†to row index.

```{r}
#| code-fold: true

node_index_lookup <- mc3_nodes_cleaned %>%
  mutate(.row_id = row_number()) %>%
  select(id, .row_id)
```

Next, the code chunk below will be used to join and convert¬†`from_id`¬†and¬†`to_id`¬†to integer indices. At the same time we also drop rows with unmatched nodes.

```{r}
#| code-fold: true

mc3_edges_indexed <- mc3_edges_cleaned %>%
  left_join(node_index_lookup, 
            by = c("from_id" = "id")) %>%
  rename(from = .row_id) %>%
  left_join(node_index_lookup, 
            by = c("to_id" = "id")) %>%
  rename(to = .row_id) %>%
  select(from, to, is_inferred, type) %>%
  filter(!is.na(from) & !is.na(to))  
```

Next the code chunk below is used to subset nodes to only those referenced by edges.

```{r}
#| code-fold: true

used_node_indices <- sort(
  unique(c(mc3_edges_indexed$from, 
           mc3_edges_indexed$to)))

mc3_nodes_final <- mc3_nodes_cleaned %>%
  slice(used_node_indices) %>%
  mutate(new_index = row_number())
```

We will then use the code chunk below to rebuild lookup from old index to new index.

```{r}
#| code-fold: true

old_to_new_index <- tibble(
  old_index = used_node_indices,
  new_index = seq_along(
    used_node_indices))
```

Lastly, the code chunk below will be used to update edge indices to match new node table.

```{r}
#| code-fold: true

mc3_edges_final <- mc3_edges_indexed %>%
  left_join(old_to_new_index, 
            by = c("from" = "old_index")) %>%
  rename(from_new = new_index) %>%
  left_join(old_to_new_index, 
            by = c("to" = "old_index")) %>%
  rename(to_new = new_index) %>%
  select(from = from_new, to = to_new, 
         is_inferred, type)
```

### **2.3 - Building the tidygraph object**

Now we are ready to build the tidygraph object by using the code chunk below.

```{r}

mc3_graph <- tbl_graph(
  nodes = mc3_nodes_final,
  edges = mc3_edges_final,
  directed = TRUE
)
```

After the tidygraph object is created, it is always a good practice to examine the object by using¬†`str()`.

```{r}

str(mc3_graph)
```

## 3 - **Exploratory Data Analysis (after cleaning & wrangling)**

Several of the¬†**ggraph**¬†layouts involve randomisation. In order to ensure reproducibility, it is necessary to set the seed value before plotting by using the code chunk below.

```{r}
set.seed(1818)
```

### **3.1 - Visualising the knowledge graph**

Shows how many nodes are of type `Entity`, `Event`, or `Relationship`.

```{r}
#| code-fold: true

mc3_nodes_final %>%
  count(type, sort = TRUE) %>%
  ggplot(aes(x = reorder(type, -n), y = n, fill = type)) +
  geom_col() +
  geom_text(aes(label = n), vjust = -0.3) +
  labs(title = "Node Type Distribution", x = "Type", y = "Count") +
  theme_minimal() +
  theme(legend.position = "none")
```

In the code chunk below,¬†**ggraph**¬†functions are used to create the whole graph.

```{r}
#| code-fold: true

ggraph(mc3_graph, 
       layout = "fr") +
  geom_edge_link(alpha = 0.3, 
                 colour = "gray") +
  geom_node_point(aes(color = `type`), 
                  size = 4) +
  geom_node_text(aes(label = type), 
                 repel = TRUE, 
                 size = 2.5) +
  theme_void()
```

### **3.2 - Entity Sub_type Distribution**

Focuses on what kinds of actors are in the graph ‚Äî Person, Vessel, Organization, etc.

```{r}
#| code-fold: true

mc3_nodes_final %>%
  filter(type == "Entity") %>%
  count(sub_type, sort = TRUE) %>%
  ggplot(aes(x = reorder(sub_type, n), y = n, fill = sub_type)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = n), hjust = -0.1) +
  labs(title = "Entity Sub-type Distribution", x = "Sub-type", y = "Count") +
  theme_minimal() +
  theme(legend.position = "none")
```

### **3.3 - Event Sub_type Distribution**

To understand what kinds of actions dominate ‚Äî Communication, Monitoring, Assessment, etc.

```{r}
#| code-fold: true

mc3_nodes_final %>%
  filter(type == "Event") %>%
  count(sub_type, sort = TRUE) %>%
  ggplot(aes(x = reorder(sub_type, n), y = n, fill = sub_type)) +
  geom_col() +
  coord_flip() +
  geom_text(aes(label = n), hjust = -0.1) +
  labs(title = "Event Sub-type Distribution", x = "Sub-type", y = "Count") +
  theme_minimal() +
  theme(legend.position = "none")

```

### **3.4a - List of Communication Participants**

This finds all Entities that **sent or received** communication events ‚Äî i.e., actors who participated in messaging.

```{r}
#| code-fold: true

library(DT)

# Step 1: Get all Communication Event IDs
comm_event_ids <- mc3_nodes_cleaned %>%
  filter(type == "Event", sub_type == "Communication") %>%
  pull(id)

# Step 2: Extract 'sent' edges for communication events
comm_sent_edges <- mc3_edges_cleaned %>%
  filter(type == "sent", to_id %in% comm_event_ids) %>%
  select(comm_id = to_id, sender_id = from_id)

# Step 3: Extract 'received' edges for same communication events
comm_received_edges <- mc3_edges_cleaned %>%
  filter(type == "received", from_id %in% comm_event_ids) %>%
  select(comm_id = from_id, receiver_id = to_id)

# Step 4: Join sent and received edges by communication ID
comm_pairs <- comm_sent_edges %>%
  inner_join(comm_received_edges, by = "comm_id")

# Step 5: Add sender and receiver labels
participants_named <- comm_pairs %>%
  left_join(mc3_nodes_cleaned %>% select(id, sender_label = label), by = c("sender_id" = "id")) %>%
  left_join(mc3_nodes_cleaned %>% select(id, receiver_label = label), by = c("receiver_id" = "id"))



# Step7: Interactive summary of top sender‚Äìreceiver pairs
participants_named %>%
  count(sender_label, receiver_label, sort = TRUE) %>%
  datatable(
    caption = "Top Communication Pairs (Sender ‚Üí Receiver)",
    colnames = c("Sender", "Receiver", "Message Count"),
    options = list(pageLength = 10, autoWidth = TRUE),
    rownames = FALSE
  )

```

### **3.4b - Network Visual of Communication Participants**

This code creates an **interactive communication network graph** using `visNetwork`, where:

-   Each **node** represents a person or entity, **node size** is based on total messages **sent** by that participant.
-   Each **edge (arrow)** represents a communication sent from one participant to another, the thicker the edge, the more message sent to that particular receiver.

**Ver 1: Layout_in_circle**

Message Senders are arranged from most to the least number of messages sent.

```{r, fig.width=10, fig.height=12}
#| code-fold: true
library(visNetwork)

# Step 1: Summarize communication edges
comm_edges_vis <- participants_named %>%
  count(sender_id, receiver_id, sort = TRUE) %>%
  rename(from = sender_id, to = receiver_id, value = n)

# Step 2: Compute messages sent per node
message_counts <- comm_edges_vis %>%
  group_by(from) %>%
  summarise(sent_count = sum(value), .groups = "drop")

# Step 3: Prepare nodes, merge with message count and add color/shape
nodes_vis <- mc3_nodes_cleaned %>%
  filter(id %in% unique(c(comm_edges_vis$from, comm_edges_vis$to))) %>%
  select(id, label, sub_type) %>%
  left_join(message_counts, by = c("id" = "from")) %>%
  mutate(
    sent_count = replace_na(sent_count, 0),
    size = rescale(sent_count, to = c(10, 40)),
    title = paste0(label, "<br>Sub-type: ", sub_type,
                   ifelse(!is.na(sent_count), paste0("<br>Sent: ", sent_count, " messages"), "")),
    color = case_when(
      sub_type == "Person" ~ "royalblue",
      sub_type == "Organization" ~ "darkorange",
      sub_type == "Vessel" ~ "forestgreen",
      sub_type == "Group" ~ "purple",
      sub_type == "Location" ~ "grey40",
      TRUE ~ "black"
    ),
    shape = case_when(
      sub_type == "Person" ~ "dot",
      sub_type == "Organization" ~ "square",
      sub_type == "Vessel" ~ "triangle",
      sub_type == "Group" ~ "star",
      sub_type == "Location" ~ "diamond",
      TRUE ~ "dot"
    )
  ) %>%
  arrange(desc(size))

# Step 4: Format visNetwork edges
edges_vis <- comm_edges_vis %>%
  mutate(
    arrows = "to",
    width = rescale(value, to = c(1, 6)),
    title = paste("Messages:", value)
  )

# Step 5: Define legend items
legend_nodes <- data.frame(
  label = c("Person", "Organization", "Vessel", "Group", "Location"),
  color = c("royalblue", "darkorange", "forestgreen", "purple", "grey40"),
  shape = c("dot", "square", "triangle", "star", "diamond"),
  stringsAsFactors = FALSE
)

# Step 6: Render network with legend
visNetwork(nodes_vis, edges_vis, width = "100%", height = "1000px") %>%
  visNodes(
    size = nodes_vis$size
    # color and shape are picked up from nodes_vis columns automatically
  ) %>%
  visLegend(
    addNodes = lapply(1:nrow(legend_nodes), function(i) {
      list(
        label = legend_nodes$label[i],
        shape = legend_nodes$shape[i],
        color = legend_nodes$color[i]
      )
    }),
    useGroups = FALSE,
    width = 0.15
  ) %>%
  visEdges(smooth = FALSE) %>%
  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%
  visIgraphLayout(layout = "layout_in_circle") %>%
  visPhysics(enabled = FALSE) %>%
  visLayout(randomSeed = 1818)

```

**Ver 2: Layout_on_sphere**

From this plot, we note that Miranda Jordan and Clepper jensen are isolated communications between themselves.s

```{r}
#| code-fold: true
library(visNetwork)

# Step 1: Summarize communication edges
comm_edges_vis <- participants_named %>%
  count(sender_id, receiver_id, sort = TRUE) %>%
  rename(from = sender_id, to = receiver_id, value = n)

# Step 2: Compute messages sent per person (by sender)
message_counts <- comm_edges_vis %>%
  group_by(from) %>%
  summarise(sent_count = sum(value), .groups = "drop")

# Step 3: Prepare nodes with label, subtype, color, shape, and scaled size
nodes_vis <- mc3_nodes_cleaned %>%
  filter(id %in% unique(c(comm_edges_vis$from, comm_edges_vis$to))) %>%
  select(id, label, sub_type) %>%
  left_join(message_counts, by = c("id" = "from")) %>%
  mutate(
    size = if_else(
      sub_type == "Person",
      rescale(sent_count, to = c(10, 40), na.rm = TRUE),
      15
    ),
    title = paste0(label, "<br>Sub-type: ", sub_type,
                   ifelse(!is.na(sent_count), paste0("<br>Sent: ", sent_count, " messages"), "")),
    color = case_when(
      sub_type == "Person" ~ "royalblue",
      sub_type == "Organization" ~ "darkorange",
      sub_type == "Vessel" ~ "forestgreen",
      sub_type == "Group" ~ "purple",
      sub_type == "Location" ~ "grey40",
      TRUE ~ "black"
    ),
    shape = case_when(
      sub_type == "Person" ~ "dot",
      sub_type == "Organization" ~ "square",
      sub_type == "Vessel" ~ "triangle",
      sub_type == "Group" ~ "star",
      sub_type == "Location" ~ "diamond",
      TRUE ~ "dot"
    )
  )

# Step 4: Format edges
edges_vis <- comm_edges_vis %>%
  mutate(
    arrows = "to",
    width = rescale(value, to = c(1, 6)),
    title = paste("Messages:", value)
  )

# Step 5: Legend mapping
legend_nodes <- data.frame(
  label = c("Person", "Organization", "Vessel", "Group", "Location"),
  color = c("royalblue", "darkorange", "forestgreen", "purple", "grey40"),
  shape = c("dot", "square", "triangle", "star", "diamond"),
  stringsAsFactors = FALSE
)

# Step 6: Render the network with layout_on_sphere and legend
visNetwork(nodes_vis, edges_vis, width = "100%", height = "900px") %>%
  visNodes(
    size = nodes_vis$size
    # color and shape columns are automatically used
  ) %>%
  visLegend(
    addNodes = lapply(1:nrow(legend_nodes), function(i) {
      list(
        label = legend_nodes$label[i],
        shape = legend_nodes$shape[i],
        color = legend_nodes$color[i]
      )
    }),
    useGroups = FALSE,
    width = 0.15
  ) %>%
  visEdges(smooth = FALSE) %>%
  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %>%
  visIgraphLayout(layout = "layout_on_sphere") %>%
  visPhysics(enabled = FALSE) %>%
  visLayout(randomSeed = 1818)


```

## 4 - Task 1a & 1b: Daily Temporal Patterns in Communications over the Two Weeks üéØ

::: callout-note
## VAST Challenge Task & Question 1a and 1b

Clepper found that messages frequently came in at around the same time each day.

-   Develop a graph-based visual analytics approach to identify any daily temporal patterns in communications.
-   How do these patterns shift over the two weeks of observations?
:::

**Objective**

1.  Identify **when** communications happen most often during each day.
2.  Detect **shifts in these patterns** over the 2-week period.
3.  Later: Focus on a **specific entity** (e.g., Nadia Conti) and explore **who influences them**.

### Step 1: Extract & Parse Communication Event Timestamps

Extract the Communication Timestamps from `mc3_nodes_final` and filter for communication events.

```{r}

# Filter for Communication events
comm_events <- mc3_nodes_final %>%
  filter(type == "Event", sub_type == "Communication") %>%
  mutate(timestamp = ymd_hms(timestamp)) %>%
  filter(!is.na(timestamp)) %>%
  mutate(
    day = as.Date(timestamp),
    hour = hour(timestamp)
  )
```

Parse the Communication Timestamp into the format "dd/mm/yyy (ddd)" for ease of reference.

```{r}

# Communication events with parsed date and time
comm_events <- mc3_nodes_final %>%
  filter(type == "Event", sub_type == "Communication") %>%
  mutate(timestamp = ymd_hms(timestamp)) %>%
  filter(!is.na(timestamp)) %>%
  mutate(
    hour = hour(timestamp),
    date_label = format(timestamp, "%d/%m/%Y (%a)")  # e.g., "19/03/2040 (Tue)"
  )

```

### Step 2: Visualize the Communication Volume for Analysis

#### **4.1 - Bar Plot of daily communication volume over the 2 weeks period:**

```{r}
#| code-fold: true

# Step 1: Prepare daily message volume data
daily_message_volume <- mc3_nodes_final %>%
  filter(type == "Event", sub_type == "Communication") %>%
  mutate(
    timestamp = ymd_hms(timestamp),
    date = as.Date(timestamp),
    date_label = format(timestamp, "%d/%m/%Y (%a)")
  ) %>%
  group_by(date, date_label) %>%
  summarise(message_count = n(), .groups = "drop") %>%
  arrange(date)

# Step 2: Compute average and total message count
avg_msg_count <- mean(daily_message_volume$message_count)
total_msg_count <- sum(daily_message_volume$message_count)

# Step 3: Plot bar chart with average + total labels
ggplot(daily_message_volume, aes(x = date_label, y = message_count)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_text(
    aes(label = message_count),
    vjust = -0.3,
    size = 2.5,
    color = "grey40"
  ) +
  geom_hline(yintercept = avg_msg_count, color = "red", linetype = "dashed", size = 1.2) +
  annotate(
    "label", x = 1, y = avg_msg_count + 2,
    label = paste("Average =", round(avg_msg_count, 1)),
    color = "red", fill = "grey90",
    label.size = 0, hjust = -0.2, vjust = 3
  ) +
  annotate(
    "label", x = nrow(daily_message_volume), y = max(daily_message_volume$message_count) + 5,
    label = paste("Total =", total_msg_count),
    color = "black", fill = "lightgrey",
    label.size = 0.3, hjust = 1.1, vjust = 1
  ) +
  labs(
    title = "Daily Radio Communication Volume",
    x = "Date",
    y = "Message Count"
  ) +
  theme_minimal(base_size = 11) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(face = "bold")
  )

```

#### **4.2 - Interactive Table of daily communication volume variation(message count)**

```{r}
#| code-fold: true
library(DT)

# Daily message volume with comparisons
daily_message_volume <- mc3_nodes_final %>%
  filter(type == "Event", sub_type == "Communication") %>%
  mutate(
    timestamp = ymd_hms(timestamp),
    date = as.Date(timestamp),
    date_label = format(timestamp, "%d/%m/%Y (%a)")
  ) %>%
  group_by(date, date_label) %>%
  summarise(message_count = n(), .groups = "drop") %>%
  arrange(date) %>%
  mutate(
    change_from_prev = message_count - lag(message_count),
    pct_change_from_prev = round((message_count - lag(message_count)) / lag(message_count) * 100, 2)
  )

datatable(
  daily_message_volume %>% select(-date),  # remove raw date if not needed
  caption = "Daily Message Volume with Day-over-Day Change",
  options = list(pageLength = 14, order = list(list(0, 'asc'))),
  rownames = FALSE
)

```

#### **4.3a - Heat Map of hourly message volume for each day over the 2 weeks period:**

This heat map is interactive and you may choose to hover on the tile to display the **date, time, and message count**

```{r}
#| code-fold: true

library(forcats)
library(plotly)

# Step 1: Reconstruct sender‚Äìreceiver‚Äìtimestamp structure
comm_events_raw <- mc3_nodes_final %>%
  filter(type == "Event", sub_type == "Communication") %>%
  select(event_id = id, timestamp) %>%
  mutate(timestamp = ymd_hms(timestamp),
         hour = hour(timestamp),
         date_label = format(timestamp, "%d/%m/%Y (%a)"))

# Step 2: Get sender (sent) and receiver (received) links
comm_edges_sent <- mc3_edges_cleaned %>%
  filter(type == "sent") %>%
  select(event_id = to_id, sender_id = from_id)

comm_edges_recv <- mc3_edges_cleaned %>%
  filter(type == "received") %>%
  select(event_id = from_id, receiver_id = to_id)

# Step 3: Join all together into sender‚Äìreceiver‚Äìtimestamp
comm_links <- comm_events_raw %>%
  left_join(comm_edges_sent, by = "event_id") %>%
  left_join(comm_edges_recv, by = "event_id") %>%
  left_join(mc3_nodes_cleaned %>% select(sender_id = id, sender_label = label), by = "sender_id") %>%
  left_join(mc3_nodes_cleaned %>% select(receiver_id = id, receiver_label = label), by = "receiver_id")

# Step 4: Aggregate total messages per hour/day
comm_heatmap <- comm_links %>%
  group_by(date_label, hour) %>%
  summarise(
    count = n(),
    top_sender = names(sort(table(sender_label), decreasing = TRUE))[1],
    sender_count = max(table(sender_label)),
    top_receiver = names(sort(table(receiver_label), decreasing = TRUE))[1],
    receiver_count = max(table(receiver_label)),
    .groups = "drop"
  ) %>%
  mutate(
    tooltip = paste0(
      "üìÖ Date: ", date_label,
      "<br>‚è∞ Hour: ", sprintf("%02d:00", hour),
      "<br>üì® Messages: ", count,
      "<br>üî¥ Top Sender: ", top_sender, " (", sender_count, ")",
      "<br>üü¢ Top Receiver: ", top_receiver, " (", receiver_count, ")"
    )
  )

# Step 5: Static ggplot
p <- ggplot(comm_heatmap, aes(
  x = hour,
  y = fct_rev(factor(date_label)),
  fill = count,
  text = tooltip
)) +
  geom_tile(color = "white") +
  scale_fill_viridis_c(option = "inferno", direction = -1, name = "Message Count") +
  scale_x_continuous(
    breaks = 0:23,
    labels = function(x) sprintf("%02d:00", x)
  ) +
  labs(
    title = "Hourly Heatmap of Radio Communications by Day",
    x = "Hour of Day",
    y = NULL
  ) +
  theme_minimal(base_size = 10) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid = element_blank()
  )

# Step 6: Make interactive
ggplotly(p, tooltip = "text")

```

We will increase the resolution to half-hour time slots.

#### **4.4b - Heat Map of half-hourly message volume for each day over the 2 weeks period:**

This heat map is interactive and you may choose to hover on the tile to display the **date, time, and message count**.

```{r}
#| code-fold: true

library(forcats)
library(plotly)

# Step 1: Fix sender and receiver edges
comm_edges_sent <- mc3_edges_cleaned %>%
  filter(type == "sent") %>%
  select(event_id = to_id, sender_id = from_id)

comm_edges_recv <- mc3_edges_cleaned %>%
  filter(type == "received") %>%
  select(event_id = from_id, receiver_id = to_id)  # ‚úÖ fixed receiver_id

# Step 2: Reconstruct sender‚Äìreceiver‚Äìevent linkage
comm_events_raw <- mc3_nodes_final %>%
  filter(type == "Event", sub_type == "Communication") %>%
  select(event_id = id, timestamp) %>%
  mutate(
    timestamp = ymd_hms(timestamp),
    hour = hour(timestamp),
    minute = minute(timestamp),
    time_bin = hour + ifelse(minute < 30, 0, 0.5),
    date_label = format(timestamp, "%d/%m/%Y (%a)"),
    time_label = sprintf("%02d:%02d", floor(time_bin), ifelse(time_bin %% 1 == 0, 0, 30))
  )

# Step 3: Join to get sender/receiver labels
comm_links <- comm_events_raw %>%
  left_join(comm_edges_sent, by = "event_id") %>%
  left_join(comm_edges_recv, by = "event_id") %>%
  left_join(mc3_nodes_cleaned %>% select(id, sender_label = label), by = c("sender_id" = "id")) %>%
  left_join(mc3_nodes_cleaned %>% select(id, receiver_label = label), by = c("receiver_id" = "id"))

# Step 4: Aggregate by half-hour + label top actors
comm_heatmap <- comm_links %>%
  group_by(date_label, time_bin, time_label) %>%
  summarise(
    count = n(),
    top_sender = names(sort(table(sender_label), decreasing = TRUE))[1],
    sender_count = max(table(sender_label)),
    top_receiver = names(sort(table(receiver_label), decreasing = TRUE))[1],
    receiver_count = max(table(receiver_label)),
    .groups = "drop"
  ) %>%
  mutate(
    tooltip = paste0(
      "üìÖ Date: ", date_label,
      "<br>üïí Time: ", time_label,
      "<br>üì® Messages: ", count,
      "<br>üî¥ Top Sender: ", top_sender, " (", sender_count, ")",
      "<br>üü¢ Top Receiver: ", top_receiver, " (", receiver_count, ")"
    )
  )

# Step 5: ggplot
p <- ggplot(comm_heatmap, aes(x = time_bin, y = fct_rev(factor(date_label)), fill = count, text = tooltip)) +
  geom_tile(color = "white") +
  scale_fill_viridis_c(
    option = "inferno",
    direction = -1,
    name = "Message Count"
  ) +
  scale_x_continuous(
    breaks = seq(0, 23.5, by = 0.5),
    labels = function(x) sprintf("%02d:%02d", floor(x), ifelse(x %% 1 == 0, 0, 30))
  ) +
  labs(
    title = "Half-Hourly Heatmap of Radio Communications by Day",
    x = "Time of Day",
    y = NULL
  ) +
  theme_minimal(base_size = 10) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    panel.grid = element_blank()
  )

# Step 6: Convert to interactive Plotly plot
ggplotly(p, tooltip = "text")
```

#### **4.4c - Density plot of Daily half-hourly message volume over the 2 weeks period:**

The faceted density plot that shows the **distribution of communication events by time of day**, broken down for each day in the dataset. It helps to visually detect **temporal communication patterns**, intensity, and consistency over multiple days.

::: panel-tabset
### Overview of the 2 week period

```{r}
#| code-fold: true
# Step 1: Preprocess communication events
comm_events <- mc3_nodes_final %>%
  filter(type == "Event", sub_type == "Communication") %>%
  mutate(timestamp = ymd_hms(timestamp)) %>%
  filter(!is.na(timestamp)) %>%
  mutate(
    date_label = format(timestamp, "%d/%m/%Y (%a)"),
    hour = hour(timestamp),
    minute = minute(timestamp),
    time_bin = hour + ifelse(minute < 30, 0, 0.5)
  )

# Step 2: Summarise daily medians and counts
daily_stats <- comm_events %>%
  group_by(date_label) %>%
  summarise(
    median_time = median(time_bin),
    msg_count = n(),
    .groups = "drop"
  )

# Step 3: Plot
ggplot(comm_events, aes(x = time_bin)) +
  geom_density(fill = "steelblue", alpha = 0.7) +
  geom_vline(data = daily_stats, aes(xintercept = median_time),
             color = "red", linetype = "dashed", linewidth = 0.6) +
  geom_text(
    data = daily_stats,
    aes(x = 20.5, y = 0.25, label = paste("Total:", msg_count)),
    inherit.aes = FALSE,
    size = 3,
    color = "grey20",
    hjust = 1
  ) +
  facet_wrap(~ date_label, ncol = 4) +
  scale_x_continuous(
    breaks = seq(0, 23.5, by = 0.5),
    labels = NULL  # suppress all x-axis labels
  ) +
  labs(
    title = "Daily Communication Patterns (Half-Hourly)",
    x = "Time of Day",
    y = "Density"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(face = "bold"),
    panel.grid.minor = element_blank()
  )

```

### Day 1 - 01/10/2040

```{r}
#| code-fold: true

# Filter for selected date
daily_data <- filter(comm_events, date_label == "01/10/2040 (Mon)")

# Build density data to find full curve
density_data <- ggplot_build(
  ggplot(daily_data, aes(x = time_bin)) +
    geom_density()
)$data[[1]]

# Identify peaks within ¬±10% of the maximum density
max_y <- max(density_data$y)
peak_threshold <- 0.9 * max_y

major_peaks <- density_data %>%
  filter(y >= peak_threshold) %>%
  group_by(grp = cumsum(c(1, diff(x) > 0.5))) %>%  # group close bins
  summarise(
    peak_time = x[which.max(y)],
    peak_density = max(y),
    .groups = "drop"
  ) %>%
  mutate(
    label = sprintf("Peak: %02d:%02d",
                    floor(peak_time),
                    ifelse(peak_time %% 1 == 0, 0, 30))
  )

# Message count
msg_count <- nrow(daily_data)

# Final plot
ggplot(daily_data, aes(x = time_bin)) +
  geom_density(fill = "steelblue", alpha = 0.7) +
  geom_vline(data = major_peaks, aes(xintercept = peak_time),
             color = "red", linetype = "dashed", linewidth = 0.6) +
  geom_text(data = major_peaks,
            aes(x = peak_time, y = 0.23, label = label),
            color = "red", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +
  annotate("text", x = 20.5, y = 0.25,
           label = paste("Total messages communicated:", msg_count),
           hjust = 1, size = 4, color = "grey30") +
  labs(
    title = "Half-Hourly Communication Density (Multiple Peaks Highlighted)",
    x = "Time of Day",
    y = "Density"
  ) +
  scale_x_continuous(
    breaks = seq(0, 23.5, by = 0.5),
    labels = function(x) sprintf("%02d:%02d", floor(x), ifelse(x %% 1 == 0, 0, 30))
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(face = "bold")
  )

```

### Day 2 - 02/10/2040

```{r}
#| code-fold: true

# Filter for selected date
daily_data <- filter(comm_events, date_label == "02/10/2040 (Tue)")

# Build density data to find full curve
density_data <- ggplot_build(
  ggplot(daily_data, aes(x = time_bin)) +
    geom_density()
)$data[[1]]

# Identify peaks within ¬±10% of the maximum density
max_y <- max(density_data$y)
peak_threshold <- 0.9 * max_y

major_peaks <- density_data %>%
  filter(y >= peak_threshold) %>%
  group_by(grp = cumsum(c(1, diff(x) > 0.5))) %>%  # group close bins
  summarise(
    peak_time = x[which.max(y)],
    peak_density = max(y),
    .groups = "drop"
  ) %>%
  mutate(
    label = sprintf("Peak: %02d:%02d",
                    floor(peak_time),
                    ifelse(peak_time %% 1 == 0, 0, 30))
  )

# Message count
msg_count <- nrow(daily_data)

# Final plot
ggplot(daily_data, aes(x = time_bin)) +
  geom_density(fill = "steelblue", alpha = 0.7) +
  geom_vline(data = major_peaks, aes(xintercept = peak_time),
             color = "red", linetype = "dashed", linewidth = 0.6) +
  geom_text(data = major_peaks,
            aes(x = peak_time, y = 0.23, label = label),
            color = "red", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +
  annotate("text", x = 20.5, y = 0.25,
           label = paste("Total messages communicated:", msg_count),
           hjust = 1, size = 4, color = "grey30") +
  labs(
    title = "Half-Hourly Communication Density (Multiple Peaks Highlighted)",
    x = "Time of Day",
    y = "Density"
  ) +
  scale_x_continuous(
    breaks = seq(0, 23.5, by = 0.5),
    labels = function(x) sprintf("%02d:%02d", floor(x), ifelse(x %% 1 == 0, 0, 30))
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(face = "bold")
  )

```

### Day 3 - 03/10/2040

```{r}
#| code-fold: true

# Filter for selected date
daily_data <- filter(comm_events, date_label == "03/10/2040 (Wed)")

# Build density data to find full curve
density_data <- ggplot_build(
  ggplot(daily_data, aes(x = time_bin)) +
    geom_density()
)$data[[1]]

# Identify peaks within ¬±10% of the maximum density
max_y <- max(density_data$y)
peak_threshold <- 0.9 * max_y

major_peaks <- density_data %>%
  filter(y >= peak_threshold) %>%
  group_by(grp = cumsum(c(1, diff(x) > 0.5))) %>%  # group close bins
  summarise(
    peak_time = x[which.max(y)],
    peak_density = max(y),
    .groups = "drop"
  ) %>%
  mutate(
    label = sprintf("Peak: %02d:%02d",
                    floor(peak_time),
                    ifelse(peak_time %% 1 == 0, 0, 30))
  )

# Message count
msg_count <- nrow(daily_data)

# Final plot
ggplot(daily_data, aes(x = time_bin)) +
  geom_density(fill = "steelblue", alpha = 0.7) +
  geom_vline(data = major_peaks, aes(xintercept = peak_time),
             color = "red", linetype = "dashed", linewidth = 0.6) +
  geom_text(data = major_peaks,
            aes(x = peak_time, y = 0.23, label = label),
            color = "red", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +
  annotate("text", x = 20.5, y = 0.25,
           label = paste("Total messages communicated:", msg_count),
           hjust = 1, size = 4, color = "grey30") +
  labs(
    title = "Half-Hourly Communication Density (Multiple Peaks Highlighted)",
    x = "Time of Day",
    y = "Density"
  ) +
  scale_x_continuous(
    breaks = seq(0, 23.5, by = 0.5),
    labels = function(x) sprintf("%02d:%02d", floor(x), ifelse(x %% 1 == 0, 0, 30))
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(face = "bold")
  )

```

### Day 4 - 04/10/2040

```{r}
#| code-fold: true

# Filter for selected date
daily_data <- filter(comm_events, date_label == "04/10/2040 (Thu)")

# Build density data to find full curve
density_data <- ggplot_build(
  ggplot(daily_data, aes(x = time_bin)) +
    geom_density()
)$data[[1]]

# Identify peaks within ¬±10% of the maximum density
max_y <- max(density_data$y)
peak_threshold <- 0.9 * max_y

major_peaks <- density_data %>%
  filter(y >= peak_threshold) %>%
  group_by(grp = cumsum(c(1, diff(x) > 0.5))) %>%  # group close bins
  summarise(
    peak_time = x[which.max(y)],
    peak_density = max(y),
    .groups = "drop"
  ) %>%
  mutate(
    label = sprintf("Peak: %02d:%02d",
                    floor(peak_time),
                    ifelse(peak_time %% 1 == 0, 0, 30))
  )

# Message count
msg_count <- nrow(daily_data)

# Final plot
ggplot(daily_data, aes(x = time_bin)) +
  geom_density(fill = "steelblue", alpha = 0.7) +
  geom_vline(data = major_peaks, aes(xintercept = peak_time),
             color = "red", linetype = "dashed", linewidth = 0.6) +
  geom_text(data = major_peaks,
            aes(x = peak_time, y = 0.23, label = label),
            color = "red", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +
  annotate("text", x = 20.5, y = 0.25,
           label = paste("Total messages communicated:", msg_count),
           hjust = 1, size = 4, color = "grey30") +
  labs(
    title = "Half-Hourly Communication Density (Multiple Peaks Highlighted)",
    x = "Time of Day",
    y = "Density"
  ) +
  scale_x_continuous(
    breaks = seq(0, 23.5, by = 0.5),
    labels = function(x) sprintf("%02d:%02d", floor(x), ifelse(x %% 1 == 0, 0, 30))
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(face = "bold")
  )

```

### Day 5 - 05/10/2040

```{r}
#| code-fold: true

# Filter for selected date
daily_data <- filter(comm_events, date_label == "05/10/2040 (Fri)")

# Build density data to find full curve
density_data <- ggplot_build(
  ggplot(daily_data, aes(x = time_bin)) +
    geom_density()
)$data[[1]]

# Identify peaks within ¬±10% of the maximum density
max_y <- max(density_data$y)
peak_threshold <- 0.9 * max_y

major_peaks <- density_data %>%
  filter(y >= peak_threshold) %>%
  group_by(grp = cumsum(c(1, diff(x) > 0.5))) %>%  # group close bins
  summarise(
    peak_time = x[which.max(y)],
    peak_density = max(y),
    .groups = "drop"
  ) %>%
  mutate(
    label = sprintf("Peak: %02d:%02d",
                    floor(peak_time),
                    ifelse(peak_time %% 1 == 0, 0, 30))
  )

# Message count
msg_count <- nrow(daily_data)

# Final plot
ggplot(daily_data, aes(x = time_bin)) +
  geom_density(fill = "steelblue", alpha = 0.7) +
  geom_vline(data = major_peaks, aes(xintercept = peak_time),
             color = "red", linetype = "dashed", linewidth = 0.6) +
  geom_text(data = major_peaks,
            aes(x = peak_time, y = 0.23, label = label),
            color = "red", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +
  annotate("text", x = 20.5, y = 0.25,
           label = paste("Total messages communicated:", msg_count),
           hjust = 1, size = 4, color = "grey30") +
  labs(
    title = "Half-Hourly Communication Density (Multiple Peaks Highlighted)",
    x = "Time of Day",
    y = "Density"
  ) +
  scale_x_continuous(
    breaks = seq(0, 23.5, by = 0.5),
    labels = function(x) sprintf("%02d:%02d", floor(x), ifelse(x %% 1 == 0, 0, 30))
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(face = "bold")
  )

```

### Day 6 - 06/10/2040

```{r}
#| code-fold: true

# Filter for selected date
daily_data <- filter(comm_events, date_label == "06/10/2040 (Sat)")

# Build density data to find full curve
density_data <- ggplot_build(
  ggplot(daily_data, aes(x = time_bin)) +
    geom_density()
)$data[[1]]

# Identify peaks within ¬±10% of the maximum density
max_y <- max(density_data$y)
peak_threshold <- 0.9 * max_y

major_peaks <- density_data %>%
  filter(y >= peak_threshold) %>%
  group_by(grp = cumsum(c(1, diff(x) > 0.5))) %>%  # group close bins
  summarise(
    peak_time = x[which.max(y)],
    peak_density = max(y),
    .groups = "drop"
  ) %>%
  mutate(
    label = sprintf("Peak: %02d:%02d",
                    floor(peak_time),
                    ifelse(peak_time %% 1 == 0, 0, 30))
  )

# Message count
msg_count <- nrow(daily_data)

# Final plot
ggplot(daily_data, aes(x = time_bin)) +
  geom_density(fill = "steelblue", alpha = 0.7) +
  geom_vline(data = major_peaks, aes(xintercept = peak_time),
             color = "red", linetype = "dashed", linewidth = 0.6) +
  geom_text(data = major_peaks,
            aes(x = peak_time, y = 0.23, label = label),
            color = "red", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +
  annotate("text", x = 20.5, y = 0.25,
           label = paste("Total messages communicated:", msg_count),
           hjust = 1, size = 4, color = "grey30") +
  labs(
    title = "Half-Hourly Communication Density (Multiple Peaks Highlighted)",
    x = "Time of Day",
    y = "Density"
  ) +
  scale_x_continuous(
    breaks = seq(0, 23.5, by = 0.5),
    labels = function(x) sprintf("%02d:%02d", floor(x), ifelse(x %% 1 == 0, 0, 30))
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(face = "bold")
  )

```

### Day 7 - 07/10/2040

```{r}
#| code-fold: true

# Filter for selected date
daily_data <- filter(comm_events, date_label == "07/10/2040 (Sun)")

# Build density data to find full curve
density_data <- ggplot_build(
  ggplot(daily_data, aes(x = time_bin)) +
    geom_density()
)$data[[1]]

# Identify peaks within ¬±10% of the maximum density
max_y <- max(density_data$y)
peak_threshold <- 0.9 * max_y

major_peaks <- density_data %>%
  filter(y >= peak_threshold) %>%
  group_by(grp = cumsum(c(1, diff(x) > 0.5))) %>%  # group close bins
  summarise(
    peak_time = x[which.max(y)],
    peak_density = max(y),
    .groups = "drop"
  ) %>%
  mutate(
    label = sprintf("Peak: %02d:%02d",
                    floor(peak_time),
                    ifelse(peak_time %% 1 == 0, 0, 30))
  )

# Message count
msg_count <- nrow(daily_data)

# Final plot
ggplot(daily_data, aes(x = time_bin)) +
  geom_density(fill = "steelblue", alpha = 0.7) +
  geom_vline(data = major_peaks, aes(xintercept = peak_time),
             color = "red", linetype = "dashed", linewidth = 0.6) +
  geom_text(data = major_peaks,
            aes(x = peak_time, y = 0.23, label = label),
            color = "red", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +
  annotate("text", x = 20.5, y = 0.25,
           label = paste("Total messages communicated:", msg_count),
           hjust = 1, size = 4, color = "grey30") +
  labs(
    title = "Half-Hourly Communication Density (Multiple Peaks Highlighted)",
    x = "Time of Day",
    y = "Density"
  ) +
  scale_x_continuous(
    breaks = seq(0, 23.5, by = 0.5),
    labels = function(x) sprintf("%02d:%02d", floor(x), ifelse(x %% 1 == 0, 0, 30))
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(face = "bold")
  )

```

### Day 8 - 08/10/2040

```{r}
#| code-fold: true

# Filter for selected date
daily_data <- filter(comm_events, date_label == "08/10/2040 (Mon)")

# Build density data to find full curve
density_data <- ggplot_build(
  ggplot(daily_data, aes(x = time_bin)) +
    geom_density()
)$data[[1]]

# Identify peaks within ¬±10% of the maximum density
max_y <- max(density_data$y)
peak_threshold <- 0.9 * max_y

major_peaks <- density_data %>%
  filter(y >= peak_threshold) %>%
  group_by(grp = cumsum(c(1, diff(x) > 0.5))) %>%  # group close bins
  summarise(
    peak_time = x[which.max(y)],
    peak_density = max(y),
    .groups = "drop"
  ) %>%
  mutate(
    label = sprintf("Peak: %02d:%02d",
                    floor(peak_time),
                    ifelse(peak_time %% 1 == 0, 0, 30))
  )

# Message count
msg_count <- nrow(daily_data)

# Final plot
ggplot(daily_data, aes(x = time_bin)) +
  geom_density(fill = "steelblue", alpha = 0.7) +
  geom_vline(data = major_peaks, aes(xintercept = peak_time),
             color = "red", linetype = "dashed", linewidth = 0.6) +
  geom_text(data = major_peaks,
            aes(x = peak_time, y = 0.23, label = label),
            color = "red", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +
  annotate("text", x = 20.5, y = 0.25,
           label = paste("Total messages communicated:", msg_count),
           hjust = 1, size = 4, color = "grey30") +
  labs(
    title = "Half-Hourly Communication Density (Multiple Peaks Highlighted)",
    x = "Time of Day",
    y = "Density"
  ) +
  scale_x_continuous(
    breaks = seq(0, 23.5, by = 0.5),
    labels = function(x) sprintf("%02d:%02d", floor(x), ifelse(x %% 1 == 0, 0, 30))
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(face = "bold")
  )

```

### Day 9 - 09/10/2040

```{r}
#| code-fold: true

# Filter for selected date
daily_data <- filter(comm_events, date_label == "09/10/2040 (Tue)")

# Build density data to find full curve
density_data <- ggplot_build(
  ggplot(daily_data, aes(x = time_bin)) +
    geom_density()
)$data[[1]]

# Identify peaks within ¬±10% of the maximum density
max_y <- max(density_data$y)
peak_threshold <- 0.9 * max_y

major_peaks <- density_data %>%
  filter(y >= peak_threshold) %>%
  group_by(grp = cumsum(c(1, diff(x) > 0.5))) %>%  # group close bins
  summarise(
    peak_time = x[which.max(y)],
    peak_density = max(y),
    .groups = "drop"
  ) %>%
  mutate(
    label = sprintf("Peak: %02d:%02d",
                    floor(peak_time),
                    ifelse(peak_time %% 1 == 0, 0, 30))
  )

# Message count
msg_count <- nrow(daily_data)

# Final plot
ggplot(daily_data, aes(x = time_bin)) +
  geom_density(fill = "steelblue", alpha = 0.7) +
  geom_vline(data = major_peaks, aes(xintercept = peak_time),
             color = "red", linetype = "dashed", linewidth = 0.6) +
  geom_text(data = major_peaks,
            aes(x = peak_time, y = 0.23, label = label),
            color = "red", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +
  annotate("text", x = 20.5, y = 0.25,
           label = paste("Total messages communicated:", msg_count),
           hjust = 1, size = 4, color = "grey30") +
  labs(
    title = "Half-Hourly Communication Density (Multiple Peaks Highlighted)",
    x = "Time of Day",
    y = "Density"
  ) +
  scale_x_continuous(
    breaks = seq(0, 23.5, by = 0.5),
    labels = function(x) sprintf("%02d:%02d", floor(x), ifelse(x %% 1 == 0, 0, 30))
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(face = "bold")
  )

```

### Day 10 - 10/10/2040

```{r}
#| code-fold: true

# Filter for selected date
daily_data <- filter(comm_events, date_label == "10/10/2040 (Wed)")

# Build density data to find full curve
density_data <- ggplot_build(
  ggplot(daily_data, aes(x = time_bin)) +
    geom_density()
)$data[[1]]

# Identify peaks within ¬±10% of the maximum density
max_y <- max(density_data$y)
peak_threshold <- 0.9 * max_y

major_peaks <- density_data %>%
  filter(y >= peak_threshold) %>%
  group_by(grp = cumsum(c(1, diff(x) > 0.5))) %>%  # group close bins
  summarise(
    peak_time = x[which.max(y)],
    peak_density = max(y),
    .groups = "drop"
  ) %>%
  mutate(
    label = sprintf("Peak: %02d:%02d",
                    floor(peak_time),
                    ifelse(peak_time %% 1 == 0, 0, 30))
  )

# Message count
msg_count <- nrow(daily_data)

# Final plot
ggplot(daily_data, aes(x = time_bin)) +
  geom_density(fill = "steelblue", alpha = 0.7) +
  geom_vline(data = major_peaks, aes(xintercept = peak_time),
             color = "red", linetype = "dashed", linewidth = 0.6) +
  geom_text(data = major_peaks,
            aes(x = peak_time, y = 0.23, label = label),
            color = "red", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +
  annotate("text", x = 20.5, y = 0.25,
           label = paste("Total messages communicated:", msg_count),
           hjust = 1, size = 4, color = "grey30") +
  labs(
    title = "Half-Hourly Communication Density (Multiple Peaks Highlighted)",
    x = "Time of Day",
    y = "Density"
  ) +
  scale_x_continuous(
    breaks = seq(0, 23.5, by = 0.5),
    labels = function(x) sprintf("%02d:%02d", floor(x), ifelse(x %% 1 == 0, 0, 30))
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(face = "bold")
  )

```

### Day 11 - 11/10/2040

```{r}
#| code-fold: true

# Filter for selected date
daily_data <- filter(comm_events, date_label == "11/10/2040 (Thu)")

# Build density data to find full curve
density_data <- ggplot_build(
  ggplot(daily_data, aes(x = time_bin)) +
    geom_density()
)$data[[1]]

# Identify peaks within ¬±10% of the maximum density
max_y <- max(density_data$y)
peak_threshold <- 0.9 * max_y

major_peaks <- density_data %>%
  filter(y >= peak_threshold) %>%
  group_by(grp = cumsum(c(1, diff(x) > 0.5))) %>%  # group close bins
  summarise(
    peak_time = x[which.max(y)],
    peak_density = max(y),
    .groups = "drop"
  ) %>%
  mutate(
    label = sprintf("Peak: %02d:%02d",
                    floor(peak_time),
                    ifelse(peak_time %% 1 == 0, 0, 30))
  )

# Message count
msg_count <- nrow(daily_data)

# Final plot
ggplot(daily_data, aes(x = time_bin)) +
  geom_density(fill = "steelblue", alpha = 0.7) +
  geom_vline(data = major_peaks, aes(xintercept = peak_time),
             color = "red", linetype = "dashed", linewidth = 0.6) +
  geom_text(data = major_peaks,
            aes(x = peak_time, y = 0.23, label = label),
            color = "red", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +
  annotate("text", x = 20.5, y = 0.25,
           label = paste("Total messages communicated:", msg_count),
           hjust = 1, size = 4, color = "grey30") +
  labs(
    title = "Half-Hourly Communication Density (Multiple Peaks Highlighted)",
    x = "Time of Day",
    y = "Density"
  ) +
  scale_x_continuous(
    breaks = seq(0, 23.5, by = 0.5),
    labels = function(x) sprintf("%02d:%02d", floor(x), ifelse(x %% 1 == 0, 0, 30))
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(face = "bold")
  )

```

### Day 12 - 12/10/2040

```{r}
#| code-fold: true

# Filter for selected date
daily_data <- filter(comm_events, date_label == "12/10/2040 (Fri)")

# Build density data to find full curve
density_data <- ggplot_build(
  ggplot(daily_data, aes(x = time_bin)) +
    geom_density()
)$data[[1]]

# Identify peaks within ¬±10% of the maximum density
max_y <- max(density_data$y)
peak_threshold <- 0.9 * max_y

major_peaks <- density_data %>%
  filter(y >= peak_threshold) %>%
  group_by(grp = cumsum(c(1, diff(x) > 0.5))) %>%  # group close bins
  summarise(
    peak_time = x[which.max(y)],
    peak_density = max(y),
    .groups = "drop"
  ) %>%
  mutate(
    label = sprintf("Peak: %02d:%02d",
                    floor(peak_time),
                    ifelse(peak_time %% 1 == 0, 0, 30))
  )

# Message count
msg_count <- nrow(daily_data)

# Final plot
ggplot(daily_data, aes(x = time_bin)) +
  geom_density(fill = "steelblue", alpha = 0.7) +
  geom_vline(data = major_peaks, aes(xintercept = peak_time),
             color = "red", linetype = "dashed", linewidth = 0.6) +
  geom_text(data = major_peaks,
            aes(x = peak_time, y = 0.23, label = label),
            color = "red", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +
  annotate("text", x = 20.5, y = 0.25,
           label = paste("Total messages communicated:", msg_count),
           hjust = 1, size = 4, color = "grey30") +
  labs(
    title = "Half-Hourly Communication Density (Multiple Peaks Highlighted)",
    x = "Time of Day",
    y = "Density"
  ) +
  scale_x_continuous(
    breaks = seq(0, 23.5, by = 0.5),
    labels = function(x) sprintf("%02d:%02d", floor(x), ifelse(x %% 1 == 0, 0, 30))
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(face = "bold")
  )

```

### Day 13 - 13/10/2040

```{r}
#| code-fold: true

# Filter for selected date
daily_data <- filter(comm_events, date_label == "13/10/2040 (Sat)")

# Build density data to find full curve
density_data <- ggplot_build(
  ggplot(daily_data, aes(x = time_bin)) +
    geom_density()
)$data[[1]]

# Identify peaks within ¬±10% of the maximum density
max_y <- max(density_data$y)
peak_threshold <- 0.9 * max_y

major_peaks <- density_data %>%
  filter(y >= peak_threshold) %>%
  group_by(grp = cumsum(c(1, diff(x) > 0.5))) %>%  # group close bins
  summarise(
    peak_time = x[which.max(y)],
    peak_density = max(y),
    .groups = "drop"
  ) %>%
  mutate(
    label = sprintf("Peak: %02d:%02d",
                    floor(peak_time),
                    ifelse(peak_time %% 1 == 0, 0, 30))
  )

# Message count
msg_count <- nrow(daily_data)

# Final plot
ggplot(daily_data, aes(x = time_bin)) +
  geom_density(fill = "steelblue", alpha = 0.7) +
  geom_vline(data = major_peaks, aes(xintercept = peak_time),
             color = "red", linetype = "dashed", linewidth = 0.6) +
  geom_text(data = major_peaks,
            aes(x = peak_time, y = 0.23, label = label),
            color = "red", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +
  annotate("text", x = 20.5, y = 0.25,
           label = paste("Total messages communicated:", msg_count),
           hjust = 1, size = 4, color = "grey30") +
  labs(
    title = "Half-Hourly Communication Density (Multiple Peaks Highlighted)",
    x = "Time of Day",
    y = "Density"
  ) +
  scale_x_continuous(
    breaks = seq(0, 23.5, by = 0.5),
    labels = function(x) sprintf("%02d:%02d", floor(x), ifelse(x %% 1 == 0, 0, 30))
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(face = "bold")
  )

```

### Day 14 - 14/10/2040

```{r}
#| code-fold: true

# Filter for selected date
daily_data <- filter(comm_events, date_label == "14/10/2040 (Sun)")

# Build density data to find full curve
density_data <- ggplot_build(
  ggplot(daily_data, aes(x = time_bin)) +
    geom_density()
)$data[[1]]

# Identify peaks within ¬±10% of the maximum density
max_y <- max(density_data$y)
peak_threshold <- 0.9 * max_y

major_peaks <- density_data %>%
  filter(y >= peak_threshold) %>%
  group_by(grp = cumsum(c(1, diff(x) > 0.5))) %>%  # group close bins
  summarise(
    peak_time = x[which.max(y)],
    peak_density = max(y),
    .groups = "drop"
  ) %>%
  mutate(
    label = sprintf("Peak: %02d:%02d",
                    floor(peak_time),
                    ifelse(peak_time %% 1 == 0, 0, 30))
  )

# Message count
msg_count <- nrow(daily_data)

# Final plot
ggplot(daily_data, aes(x = time_bin)) +
  geom_density(fill = "steelblue", alpha = 0.7) +
  geom_vline(data = major_peaks, aes(xintercept = peak_time),
             color = "red", linetype = "dashed", linewidth = 0.6) +
  geom_text(data = major_peaks,
            aes(x = peak_time, y = 0.23, label = label),
            color = "red", size = 3.5, hjust = -0.1, inherit.aes = FALSE) +
  annotate("text", x = 20.5, y = 0.25,
           label = paste("Total messages communicated:", msg_count),
           hjust = 1, size = 4, color = "grey30") +
  labs(
    title = "Half-Hourly Communication Density (Multiple Peaks Highlighted)",
    x = "Time of Day",
    y = "Density"
  ) +
  scale_x_continuous(
    breaks = seq(0, 23.5, by = 0.5),
    labels = function(x) sprintf("%02d:%02d", floor(x), ifelse(x %% 1 == 0, 0, 30))
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    strip.text = element_text(face = "bold")
  )

```
:::

üìà **Insights This Visualization Offers**

### Step 3: Plot Combined Hourly and Half-hourly Communication Volume

**Bar Plot of combined hourly message volume over the 2 weeks period:**

```{r}
#| code-fold: true

# Prepare data
comm_hourly <- comm_events %>%
  count(hour) %>%
  mutate(
    hour_label = sprintf("%02d:00", hour),  # Format to hh:mm
    percent = n / sum(n)
  )

# Plot
ggplot(comm_hourly, aes(x = hour_label, y = n)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_text_repel(
    aes(label = paste0(n, " (", percent(percent, accuracy = 1), ")")),
    nudge_y = 3,
    size = 2.5,
    direction = "y",
    max.overlaps = Inf
  ) +
  labs(
    title = "Overall Hourly Communication Volume",
    x = "Time of Day (hh:mm)",
    y = "Message Count"
  ) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(face = "bold")
  )

```

**Bar Plot of combined half-hourly message volume in the 2 weeks period.**

```{r}
#| code-fold: true

comm_events <- mc3_nodes_final %>%
  filter(type == "Event", sub_type == "Communication") %>%
  mutate(timestamp = ymd_hms(timestamp)) %>%
  filter(!is.na(timestamp)) %>%
  mutate(
    hour = hour(timestamp),
    minute = minute(timestamp),
    time_bin = sprintf("%02d:%02d", hour, ifelse(minute < 30, 0, 30))
  )

comm_halfhour <- comm_events %>%
  count(time_bin) %>%
  mutate(percent = n / sum(n))

ggplot(comm_halfhour, aes(x = time_bin, y = n)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  geom_text_repel(
    aes(label = paste0(n, " (", percent(percent, accuracy = 1), ")")),
    nudge_y = 3,
    size = 2.5,
    direction = "y",
    max.overlaps = Inf
  ) +
  labs(
    title = "Overall Half-Hourly Communication Volume",
    x = "Time of Day (hh:mm)",
    y = "Message Count"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    plot.title = element_text(face = "bold")
  )

```

::: callout-tip
## 1a. What are the identifiable daily temporal patterns in communications?

1.  The daily communication volume fluctuates slightly between 34 and 49 messages, with an average of approximately 42 messages per day, highlighting a stable overall activity level. Notably, the highest volume occurs on 11th October (49 messages), immediately following the lowest volume the day before on 10th October (34 messages)‚Äîa sharp rebound that may signal a response to specific events or operational needs. Despite these fluctuations, the system maintains a consistent tempo across the two weeks.

2.  The temporal analysis using both the heat map and time series plots reveals a pronounced morning-centric communication rhythm. The vast majority of radio traffic is concentrated between 9:00 AM and 11:30 AM, with the most intense peaks typically occurring between 10:00 and 11:00 AM. With reference to the Density plot of Daily half-hourly message volume, of the 14 days, we see message density peaks at 10:30 AM on 9 days, while on 3 days, it peaks at 12:30 PM.

3.  For instance if we were to based in on the hourly plot, 5th October (Fri) and 11th October (Thu) both register their highest single-hour counts at 10:00 AM at 24 and 21 messages respectively. Communication activity drops off steeply after lunchtime, with more than 90% of the days showing little to no activity after 2:30 PM. This pattern suggests a highly structured daily workflow, where key decisions and coordination are front-loaded in the day. Importantly, the hourly heat map also indicates that this routine holds across both weekdays and weekends‚Äîcommunication volumes and peak hours remain similar, underlining the operational regularity of the group regardless of the day of week.
:::

::: callout-tip
## 1b. How do these patterns shift over the two weeks of observations?

1.  Over the two-week period, while the timing and structure of communication peaks remain broadly consistent, there are subtle shifts in both intensity and timing. Some days, such as 3rd, 5th, 11th and 12th October, see particularly high spikes in the mid-morning, which may correspond to critical events, decision points, or heightened urgency. The sharp dip on October 8th and 13th, immediately after a period of "surge" (3rd - 7th and 9th to 12th October), points to possible responses to interruptions, lulls, or triggering incidents. Overall, although the daily messaging routine is remarkably stable, these bursts and brief lulls provide clues to changing circumstances or stress points in the operation‚Äîan analytical signal that warrants closer inspection of event logs or external triggers for those dates.

2.  Another notable change in the communication pattern is observed during the weekends. In the first week, weekend communication peaks occurred earlier, typically between 10:00 AM and 11:30 AM, closely mirroring the weekday rhythm. However, in the second week, the weekend peaks shifted noticeably later, with the highest message volumes concentrated around 12:00 PM and 1:00 PM. This shift not only marks a departure from the otherwise stable early-morning communication structure but also suggests an adaptive or reactive operational schedule‚Äîpotentially in response to evolving events, increased coordination needs, or changing priorities as the observation period progressed. The contrast between the two weekends is clear in the heatmap, underscoring the importance of monitoring such shifts as possible indicators of underlying changes in group behavior or external pressures.
:::

## 5 - Task 1c: Focus on a Particular Entity - "Nadia Conti"

::: callout-note
## VAST Challenge Task & Question 1c

Clepper found that messages frequently came in at around the same time each day.

1.  Focus on a specific entity and use this information to determine who has influence over them.
:::

### 5.1 Data Preparation for "Nadia Conti" Influence Analysis

We first extracted the relevant communication edges from the dataset, pairing ‚Äúsent‚Äù and ‚Äúreceived‚Äù communication events to form entity-to-entity links. We retained only those edges where both nodes represent real-world entities (Person, Organization, Vessel, Group, or Location), ensuring that our analysis focuses on the meaningful actors in the Oceanus network.

```{r}
#| code-fold: true

# Extract sent and received communication event edges
sent_edges <- mc3_edges_cleaned %>%
  filter(type == "sent") %>%
  select(source_entity = from_id, event = to_id)

received_edges <- mc3_edges_cleaned %>%
  filter(type == "received") %>%
  select(event = from_id, target_entity = to_id)

# Pair sent and received to form communication edges
paired_edges <- sent_edges %>%
  inner_join(received_edges, by = "event") %>%
  select(from = source_entity, to = target_entity)

# Add unmatched sent and received edges (optional, for completeness)
single_sent_edges <- sent_edges %>%
  select(from = source_entity, to = event)
single_received_edges <- received_edges %>%
  select(from = event, to = target_entity)

all_edges <- bind_rows(paired_edges, single_sent_edges, single_received_edges) %>%
  distinct()

# Identify entity nodes (Person, Organization, Vessel, Group, Location)
entity_ids <- mc3_nodes_cleaned %>%
  filter(sub_type %in% c("Person", "Organization", "Vessel", "Group", "Location")) %>%
  pull(id) %>% as.character()

entity_edges <- all_edges %>%
  filter(as.character(from) %in% entity_ids, as.character(to) %in% entity_ids)

entity_nodes <- mc3_nodes_cleaned %>%
  filter(sub_type %in% c("Person", "Organization", "Vessel", "Group", "Location")) %>%
  select(id, label, sub_type)

```

### 5.2 Build the Global Network and Compute Centrality

Using these cleaned and filtered edges and nodes, we built a global directed graph representing the Oceanus community. We then computed key network centrality metrics for each node‚ÄîPageRank, betweenness, and degree‚Äîquantifying the influence and connectivity of every entity in the overall network.

```{r}
#| code-fold: true
library(igraph)

g <- graph_from_data_frame(d = entity_edges, vertices = entity_nodes, directed = TRUE)

# Compute centralities
V(g)$pagerank <- page_rank(g)$vector
V(g)$betweenness <- betweenness(g)
V(g)$degree <- degree(g)

```

### 5.3 Extract "Nadia Conti" Ego Network (2-hop Neighbourhood)

Focusing on "Nadia Conti", we identified her node and extracted her two-step ego network, capturing both direct and indirect connections within the broader network. This local subgraph reveals Nadia‚Äôs immediate sphere of influence and the key players connected to her.

```{r}
#| code-fold: true
nadia_label <- "Nadia Conti"
target_index <- which(V(g)$label == nadia_label)

ego_graph <- make_ego_graph(g, order = 2, nodes = target_index, mode = "all")[[1]]

```

### 5.4 Visualize Nadia Conti‚Äôs Ego Network (Interactive)

We visualized Nadia‚Äôs ego network using node size, shape, and color to represent centrality and entity type. We also summarized centrality metrics in clear tables, ranking all ego network members by PageRank, Betweenness, and Degree. This allows for direct identification of the most influential, best-connected, and most strategic actors in Nadia Conti‚Äôs communication environment.

```{r}
#| code-fold: true

nodes_df <- data.frame(
  id = V(ego_graph)$name,
  label = V(ego_graph)$label,
  group = V(ego_graph)$sub_type,
  title = paste0("<b>", V(ego_graph)$label, "</b><br>",
                 "Degree: ", round(V(ego_graph)$degree, 2), "<br>",
                 "Betweenness: ", round(V(ego_graph)$betweenness, 2), "<br>",
                 "PageRank: ", round(V(ego_graph)$pagerank, 4)),
  shape = ifelse(V(ego_graph)$sub_type == "Person", "dot",
                 ifelse(V(ego_graph)$sub_type == "Organization", "square",
                        ifelse(V(ego_graph)$sub_type == "Vessel", "triangle",
                               ifelse(V(ego_graph)$sub_type == "Group", "star", "diamond")))),
  value = V(ego_graph)$pagerank * 30 + 5
)

edges_df <- as_data_frame(ego_graph, what = "edges") %>%
  rename(from = from, to = to)

library(visNetwork)
visNetwork(nodes_df, edges_df, width = "100%", height = "700px") %>%
  visNodes(scaling = list(min = 5, max = 30)) %>%
  visEdges(
    arrows = list(to = list(enabled = TRUE, scaleFactor = 0.3)),
    color = list(color = "gray")
  ) %>%
  visOptions(
    highlightNearest = TRUE,
    nodesIdSelection = TRUE,
    manipulation = FALSE
  ) %>%
  visInteraction(
    dragNodes = FALSE,
    dragView = FALSE,
    zoomView = FALSE
  ) %>%
  visLegend() %>%
  visLayout(randomSeed = 1818)

```

### 5.5 Centrality Tables for Nadia‚Äôs Ego Network

On both the global and Nadia-focused ego networks, we computed standard network centrality metrics for all nodes:

-   **PageRank** (overall influence),
-   **Betweenness** (information brokerage/intermediary role), and
-   **Degree** (number of direct connections).

These measures quantify the importance and structural roles of each entity relative to Nadia and the broader community.

```{r}
#| code-fold: true

# PageRank table
pagerank_df <- data.frame(
  label = V(ego_graph)$label,
  sub_type = V(ego_graph)$sub_type,
  pagerank = round(V(ego_graph)$pagerank, 4)
) %>% arrange(desc(pagerank))

# Betweenness table
betweenness_df <- data.frame(
  label = V(ego_graph)$label,
  sub_type = V(ego_graph)$sub_type,
  betweenness = round(V(ego_graph)$betweenness, 2)
) %>% arrange(desc(betweenness))

# Degree table
degree_df <- data.frame(
  label = V(ego_graph)$label,
  sub_type = V(ego_graph)$sub_type,
  degree = V(ego_graph)$degree
) %>% arrange(desc(degree))


```

```{r}
knitr::kable(pagerank_df, caption = "PageRank Centrality (Nadia's Ego Network)")
```

```{r}
knitr::kable(betweenness_df, caption = "Betweenness Centrality (Nadia's Ego Network)")
```

```{r}
knitr::kable(degree_df, caption = "Degree Centrality (Nadia's Ego Network)")
```

## Unused Plots

```{r}

pagerank_df <- data.frame(
  label = V(ego_graph)$label,
  sub_type = V(ego_graph)$sub_type,
  pagerank = round(V(ego_graph)$pagerank, 4)
) %>% arrange(desc(pagerank))

betweenness_df <- data.frame(
  label = V(ego_graph)$label,
  sub_type = V(ego_graph)$sub_type,
  betweenness = round(V(ego_graph)$betweenness, 2)
) %>% arrange(desc(betweenness))

degree_df <- data.frame(
  label = V(ego_graph)$label,
  sub_type = V(ego_graph)$sub_type,
  degree = V(ego_graph)$degree
) %>% arrange(desc(degree))

```
